<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="teaser_iccv.png" />



  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>IntrinsicControlNet: Cross-distribution Image Generation with Real and Unreal</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">IntrinsicControlNet: Cross-distribution Image Generation with Real and Unreal</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jiayuan Lu</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Rengan Xie</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zixuan Xie</a><sup>2</sup>,</span>
                    <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhizhen Wu</a><sup>1</sup>,</span>
                      <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Dianbing Xi</a><sup>1</sup>,</span>
                        <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Qi Ye</a><sup>3</sup>,</span>
                          <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Rui Wang</a><sup>1</sup>,</span>
                            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hujun Bao</a><sup>1</sup>,</span>
                              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuchi Huo</a><sup>1,4†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>State Key Lab of CAD&CG, Zhejiang University<br><sup>2</sup>Institute of Computing Technology, Chinese Academy of Sciences<br><sup>3</sup>State Key Laboratory of Industrial Control Technology, Zhejiang University<br><sup>4</sup>Zhejiang Lab<br>ICCV 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="teaser_iccv.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        We propose an intrinsic controllable generative framework that synthesizes realistic images from 3D assets, similar to a rendering engine, using intrinsic images like material, geometric, and lighting information as network inputs. This approach greatly simplifies intrinsic editing tasks for real images, such as object insertion and removal, while achieving better results compared to existing methods.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Realistic images are usually produced by simulating light transportation results of 3D scenes using rendering engines. This framework can precisely control the output but is usually weak at producing photo-like images. Alternatively, diffusion models have seen great success in photorealistic image generation by leveraging priors from large datasets of real-world images but lack affordance controls. Promisingly, the recent ControlNet enables flexible control of the diffusion model without degrading its generation quality. In this work, we introduce IntrinsicControlNet, an intrinsically controllable image generation framework that enables easily generating photorealistic images from precise and explicit control, similar to a rendering engine, by using intrinsic images such as material properties, geometric details, and lighting as network inputs.
Beyond this, we notice that there is a domain gap between the synthetic and real-world datasets, and therefore, naively blending these datasets yields domain confusion. To address this problem, we present a cross-domain control architecture that extracts control information from synthetic datasets, and control and content information from real-world datasets. This bridges the domain gap between real-world and synthetic datasets, enabling the blending or editing of 3D assets and real-world photos to support various interesting applications.  Experiments and user studies demonstrate that our method can generate explicitly controllable and highly photorealistic images based on the input intrinsic images.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="overview_iccv_cr.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of our pipeline. Given the intrinsic images from the graphic engine (a), the other predicted from refined Intrinsic Image Diffusion (b) and predicted HDR panoramic environment map from StyleLight (c), they are divided into two groups, material and geometry. We depict the synthetic and real data flows using dashed and solid lines, respectively. The input pass through a matrix switch to form the comprehensive control conditions (Section 3.2). Then, a tailored conditional encoder is employed to extract the diverse condition features. The features are fed separately into two ControlNets, each controlling the latent diffusion model to generate material and geometry information for the final realistic images. These are then followed by a realistic discriminator that further enhances the realism of the generated results. (Section 3.4).
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="cross-domain_iccv.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          In-distribution Generation (existing): a) generate synthetic RGB (rendered by engines) from synthetic intrinsic inputs (generated from 3D models); b) generate real-world RGB (photo dataset) from real intrinsic inputs (generated by intrinsic decomposition models). Cross-distribution Generation (ours): map both real-world and synthetic intrinsic inputs to a unified distribution and generate real-world RGB from it, thus enabling the blending of real and unreal.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="rgbx_domain_iccv_sr.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Insertion of objects with different distributions: We fused a real object (Scene1) with a synthetic background (Scene2). The first column shows the simple addition of the reference scenes, while the right three columns display the results of using RGB↔X and our method separately on each scene and the combined scene. While RGB↔X works well for same-distribution tasks, it struggles with mixed distributions. For instance, the texture of the coffee table becomes smoother, and artifacts appear in the background image and coffee cup in the red boxes. Our method, however, enhances the realism of Scene2 and casts accurate shadows of Scene1, excelling in cross-distribution tasks.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="ablation_w_light_sr.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Ablation study. We compare our full method with variations in our architecture design and training strategies. Training solely on the synthetic dataset results in unrealistic exposure and colors, while training only on the real dataset fails to capture precise structures. Additionally, merging the geometry and material branches into a single path causes noticeable confusion, such as the clothing appearing as grass. Not using the realistic discriminator results in outputs that fall between realistic and synthetic, rather than producing highly realistic images. Excluding lighting results in random lighting effects in the generated outputs. Our full method generates more realistic details while maintaining geometry and material features, such as the shadow of the column under sunlight in the red box.
      </h2>
    </div>
      <div class="item">
      <!-- Your image here -->
      <img src="addition-col_sr.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Object insertion, the blue boxes of object insertion in the first column represent real-world images, while the red boxes represent synthesized images.
      </h2>
    </div>
      <div class="item">
      <!-- Your image here -->
      <img src="edit_iccv_sr.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Edition results of real-world and synthetic scenes.
      </h2>
    </div>
      <div class="item">
      <!-- Your image here -->
      <img src="baseline_light_sr.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison results of Ours, Multi-ControlNet, RGB↔X, and Ctrl-X. 'Multi.' refers to Multi-ControlNet.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
